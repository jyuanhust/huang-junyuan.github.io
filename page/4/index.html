<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="yuan" href="https://huang-junyuan.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="yuan" href="https://huang-junyuan.github.io/atom.xml"><link rel="alternate" type="application/json" title="yuan" href="https://huang-junyuan.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://huang-junyuan.github.io/page/4/"><title>Mi Manchi = yuan = Whatever is worth doing at all is worth doing well</title><meta name="generator" content="Hexo 6.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><a href="/" class="logo" rel="start"><p class="artboard">Mi Manchi</p><h1 itemprop="name headline" class="title">yuan</h1></a><p class="meta" itemprop="description">= Whatever is worth doing at all is worth doing well =</p></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Mi Manchi</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(91).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(98).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(19).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(54).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(52).webp"></li><li class="item" data-background-image="https://gitee.com/zkz0/image/raw/master/img/img(65).webp"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="index wrap"><div class="segments posts"><article class="item"><div class="cover"><a href="/2022/11/12/backend/SpringMVC/SpringMVC_day02/SpringMVC_day02/" itemprop="url" title="SpringMVC_day02"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(71).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 22:42:40"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T22:42:40+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>42k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>38 分钟</span></span></div><h3><a href="/2022/11/12/backend/SpringMVC/SpringMVC_day02/SpringMVC_day02/" itemprop="url" title="SpringMVC_day02">SpringMVC_day02</a></h3><div class="excerpt"># SpringMVC_day02 今日内容 完成 SSM 的整合开发 能够理解并实现统一结果封装与统一异常处理 能够完成前后台功能整合开发 掌握拦截器的编写 # 1，SSM 整合 前面我们已经把 Mybatis 、 Spring 和 SpringMVC 三个框架进行了学习，今天主要的内容就是把这三个框架整合在一起完成我们的业务功能开发，具体如何来整合，我们一步步来学习。 # 1.1 流程分析 (1) 创建工程 创建一个 Maven 的 web 工程 pom.xml 添加 SSM 需要的依赖 jar 包 编写 Web 项目的入口配置类，实现...</div><div class="meta footer"><span><a href="/categories/backend/spring/SpringMVC/" itemprop="url" title="SpringMVC"><i class="ic i-flag"></i>SpringMVC</a></span></div><a href="/2022/11/12/backend/SpringMVC/SpringMVC_day02/SpringMVC_day02/" itemprop="url" title="SpringMVC_day02" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" itemprop="url" title="Byte-Pair Encoding tokenization"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(44).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>7.3k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>7 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" itemprop="url" title="Byte-Pair Encoding tokenization">Byte-Pair Encoding tokenization</a></h3><div class="excerpt"># Byte-Pair Encoding tokenization 字节对编码 (BPE) 最初被开发为一种压缩文本的算法，然后在预训练 GPT 模型时被 OpenAI 用于标记化。许多 Transformer 模型都使用它，包括 GPT、GPT-2、RoBERTa、BART 和 DeBERTa。 # 训练算法 BPE 训练首先计算语料库中使用的唯一单词集 (在完成标准化和预标记化步骤之后), 然后通过获取用于编写这些单词的所有符号来构建词汇表。一个非常简单的例子，假设我们的语料库使用了这五个词: &quot;hug&quot;, &quot;pug&quot;,...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" itemprop="url" title="Byte-Pair Encoding tokenization" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/Unigram%E6%A0%87%E8%AE%B0%E5%8C%96/" itemprop="url" title="Unigram"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(10).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>104</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/Unigram%E6%A0%87%E8%AE%B0%E5%8C%96/" itemprop="url" title="Unigram">Unigram</a></h3><div class="excerpt"># Unigram 标记化 Unigram tokenization 在 SentencePiece 中经常使用 Unigram 算法，该算法是 AlBERT、T5、mBART、Big Bird 和 XLNet 等模型使用的标记化算法。</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/Unigram%E6%A0%87%E8%AE%B0%E5%8C%96/" itemprop="url" title="Unigram" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/NormalizationAndPre-tokenization/" itemprop="url" title="Normalization and pre-tokenization"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(21).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>371</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/NormalizationAndPre-tokenization/" itemprop="url" title="Normalization and pre-tokenization">Normalization and pre-tokenization</a></h3><div class="excerpt"># Normalization and pre-tokenization 标准化和预标记化 在我们更深入地研究与 Transformer 模型（字节对编码 Byte-Pair Encoding [BPE]、WordPiece 和 Unigram）一起使用的三种最常见的子词标记化算法之前，我们将首先看一下每个标记器 tokenizer 应用于文本的预处理。以下是 tokenization pipeline 标记化管道 中步骤的高级概述： 在将文本拆分为子标记之前（根据其模型），分词器执行两个步骤： normalization 和 pre-tokenization. #...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/NormalizationAndPre-tokenization/" itemprop="url" title="Normalization and pre-tokenization" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/note/" itemprop="url" title="NLP和transformer大类概述"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(91).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>192</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/note/" itemprop="url" title="NLP和transformer大类概述">NLP和transformer大类概述</a></h3><div class="excerpt">对于 head 的理解，预训练的模型可能自己本身是带有 head 的，如果使用 autoModel 的话，那么就会自动加上这个 Head。但如果想要利用这个预训练模型，然后调整下游任务，那么就要换上特定的 token，这是就得使用 AutoModelFor...，这样模型就会自动替换原先的 head，然后就可以从头开始训练了。 但是因为还是使用原来的模型，所以 AutoTokenizer 还是可以继续用的。</div><div class="meta footer"><span><a href="/categories/ai/huggingface/" itemprop="url" title="huggingface"><i class="ic i-flag"></i>huggingface</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/note/" itemprop="url" title="NLP和transformer大类概述" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/%E6%A0%B9%E6%8D%AE%E5%B7%B2%E6%9C%89%E7%9A%84tokenizer%E8%AE%AD%E7%BB%83%E6%96%B0%E7%9A%84tokenizer/" itemprop="url" title="根据已有的tokenizer训练新的tokenizer"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(41).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>975</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/%E6%A0%B9%E6%8D%AE%E5%B7%B2%E6%9C%89%E7%9A%84tokenizer%E8%AE%AD%E7%BB%83%E6%96%B0%E7%9A%84tokenizer/" itemprop="url" title="根据已有的tokenizer训练新的tokenizer">根据已有的tokenizer训练新的tokenizer</a></h3><div class="excerpt"># 根据已有的 tokenizer 训练新的 tokenizer 如果您感兴趣的语言中没有可用的语言模型，或者如果您的语料库与您的语言模型所训练的语料库有很大不同，您很可能希望从适合您的数据的标记器从头开始重新训练模型。这将需要在您的数据集上训练一个新的标记器。 但这究竟是什么意思？ 当我们在 第二章 中第一次查看标记器时，我们看到大多数 Transformer 模型使用子词分词算法。 为了识别哪些子词是感兴趣的并且在手头的语料库中最常出现，标记器需要仔细查看语料库中的所有文本 —— 我们称之为 training 的过程。...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/%E6%A0%B9%E6%8D%AE%E5%B7%B2%E6%9C%89%E7%9A%84tokenizer%E8%AE%AD%E7%BB%83%E6%96%B0%E7%9A%84tokenizer/" itemprop="url" title="根据已有的tokenizer训练新的tokenizer" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/WordPiece/" itemprop="url" title="WordPiece"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(15).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>848</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/WordPiece/" itemprop="url" title="WordPiece">WordPiece</a></h3><div class="excerpt"># WordPiece 标记化 WordPiece 是 Google 为预训练 BERT 而开发的标记化算法。此后，它在不少基于 BERT 的 Transformer 模型中得到重用，例如 DistilBERT、MobileBERT、Funnel Transformers 和 MPNET。它在训练方面与 BPE 非常相似，但实际标记化的方式不同。 # 训练算法 Google 从未开源 WordPiece 训练算法的实现，因此以下是我们基于已发表文献的最佳猜测。它可能不是 100% 准确的。 与 BPE 一样，WordPiece...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/WordPiece/" itemprop="url" title="WordPiece" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" itemprop="url" title="问答 question answer"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(89).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>18k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>16 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" itemprop="url" title="问答 question answer">问答 question answer</a></h3><div class="excerpt"># 问答 Question answering 是时候看问答了！这项任务有多种形式，但我们将在本节中关注的一项称为提取的问答 extractive question answering。问题的答案就在 给定的文档 之中。 我们将使用 SQuAD 数据集 微调一个 BERT 模型，其中包括群众工作者对一组维基百科文章提出的问题。 像 BERT 这样的纯编码器模型往往很擅长提取诸如 “谁发明了 Transformer 架构？” 之类的事实性问题的答案。但在给出诸如 “为什么天空是蓝色的？” 之类的开放式问题时表现不佳。在这些更具挑战性的情况下，T5 和 BART 等编码器 -...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81nlp%E4%BB%BB%E5%8A%A1/" itemprop="url" title="主要nlp任务"><i class="ic i-flag"></i>主要nlp任务</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" itemprop="url" title="问答 question answer" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" itemprop="url" title="微调一个掩码语言模型"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(73).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>18k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>16 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" itemprop="url" title="微调一个掩码语言模型">微调一个掩码语言模型</a></h3><div class="excerpt"># 微调掩码语言模型 对于许多涉及 Transformer 模型的 NLP 程序，你可以简单地从 Hugging Face Hub 中获取一个预训练的模型，然后直接在你的数据上对其进行微调，以完成手头的任务。只要用于预训练的语料库与用于微调的语料库没有太大区别，迁移学习通常会产生很好的结果。 但是，在某些情况下，你需要先微调数据上的语言模型，然后再训练特定于任务的 head。例如，如果您的数据集包含法律合同或科学文章，像 BERT 这样的普通 Transformer...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81nlp%E4%BB%BB%E5%8A%A1/" itemprop="url" title="主要nlp任务"><i class="ic i-flag"></i>主要nlp任务</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" itemprop="url" title="微调一个掩码语言模型" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" itemprop="url" title="微调一个预训练模型-处理数据"><img data-src="https://gitee.com/zkz0/image/raw/master/img/img(69).webp"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>11k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>10 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" itemprop="url" title="微调一个预训练模型-处理数据">微调一个预训练模型-处理数据</a></h3><div class="excerpt"># 处理数据 下面是我们用模型中心的数据在 PyTorch 上训练句子分类器的一个例子： import torchfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification# Same as beforecheckpoint = &quot;bert-base-uncased&quot;tokenizer = AutoTokenizer.from_pretrained(checkpoint)model =...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" itemprop="url" title="微调一个预训练模型"><i class="ic i-flag"></i>微调一个预训练模型</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" itemprop="url" title="微调一个预训练模型-处理数据" class="btn">more...</a></div></article></div></div><nav class="pagination"><div class="inner"><a class="extend prev" rel="prev" href="/page/3/"><i class="ic i-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><a class="extend next" rel="next" href="/page/5/"><i class="ic i-angle-right" aria-label="下一页"></i></a></div></nav></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"></div><div class="related panel pjax" data-title="系列文章"></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="yuan" data-src="/images/avatar.jpg"><p class="name" itemprop="name">yuan</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">223</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">47</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">39</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item email" data-url="bWFpbHRvOjIwODM2MzU1MjVAcXEuY29t" title="mailto:2083635525@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>链接</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/page/3/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/page/5/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/" title="分类于 huggingface">huggingface</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" title="分类于 Tokenizer库">Tokenizer库</a></div><span><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" title="Byte-Pair Encoding tokenization">Byte-Pair Encoding tokenization</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/tools/" title="分类于 tools">tools</a> <i class="ic i-angle-right"></i> <a href="/categories/tools/%E7%88%AC%E8%99%AB/" title="分类于 爬虫">爬虫</a></div><span><a href="/2022/08/26/tools/%E7%88%AC%E8%99%AB/xpath/" title="xpath">xpath</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/backend/" title="分类于 后端">后端</a> <i class="ic i-angle-right"></i> <a href="/categories/backend/spring/" title="分类于 spring">spring</a> <i class="ic i-angle-right"></i> <a href="/categories/backend/spring/SpringMVC/" title="分类于 SpringMVC">SpringMVC</a></div><span><a href="/2022/11/12/backend/SpringMVC/SpringMVC_day01/SpringMVC_day01/" title="SpringMVC_day01">SpringMVC_day01</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/" title="分类于 nlp">nlp</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/" title="分类于 huggingface">huggingface</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" title="分类于 Tokenizer库">Tokenizer库</a></div><span><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/NormalizationAndPre-tokenization/" title="Normalization and pre-tokenization">Normalization and pre-tokenization</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/frontend/" title="分类于 前端">前端</a> <i class="ic i-angle-right"></i> <a href="/categories/frontend/Node-js/" title="分类于 Node.js">Node.js</a></div><span><a href="/2022/09/08/frontend/Node/Node%E6%A8%A1%E5%9D%97%E5%8C%96/" title="Node模块化">Node模块化</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/cv/" title="分类于 cv">cv</a></div><span><a href="/2022/08/25/ai/cv/MobileNet/" title="MobileNet">MobileNet</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/frontend/" title="分类于 前端">前端</a> <i class="ic i-angle-right"></i> <a href="/categories/frontend/javascript/" title="分类于 javascript">javascript</a></div><span><a href="/2022/07/25/frontend/javascript/%E6%95%B0%E7%BB%84-JS/" title="数组-JavaScript">数组-JavaScript</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/frontend/" title="分类于 前端">前端</a> <i class="ic i-angle-right"></i> <a href="/categories/frontend/vue/" title="分类于 vue">vue</a> <i class="ic i-angle-right"></i> <a href="/categories/frontend/vue/mediapipe-vue/" title="分类于 mediapipe-vue">mediapipe-vue</a></div><span><a href="/2022/09/18/frontend/mediapine-vue/%E5%A7%BF%E6%80%81%E6%A3%80%E6%B5%8B/" title="姿态检测">姿态检测</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2022/08/24/ai/base/%E6%95%B0%E5%AD%A6-%E5%B8%B8%E8%A7%81%E5%87%BD%E6%95%B0%E6%B1%82%E5%AF%BC%E8%BF%87%E7%A8%8B/" title="数学-常见函数求导过程">数学-常见函数求导过程</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 computer-science">computer-science</a></div><span><a href="/2023/03/01/computer-science/%E4%BF%9D%E7%A0%94/note2%20copy%202/" title="测试同步4">测试同步4</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">yuan @ Mi Manchi</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">1.6m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">23:49</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"page/4/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>