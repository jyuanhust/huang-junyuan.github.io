<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="yuan" href="https://huang-junyuan.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="yuan" href="https://huang-junyuan.github.io/atom.xml"><link rel="alternate" type="application/json" title="yuan" href="https://huang-junyuan.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><link rel="canonical" href="https://huang-junyuan.github.io/page/2/"><title>Mi Manchi = yuan = Whatever is worth doing at all is worth doing well</title><meta name="generator" content="Hexo 6.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><a href="/" class="logo" rel="start"><p class="artboard">Mi Manchi</p><h1 itemprop="name headline" class="title">yuan</h1></a><p class="meta" itemprop="description">= Whatever is worth doing at all is worth doing well =</p></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Mi Manchi</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclffsa1cj20zk0m811l.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclhpw3lwj20zk0m8gvw.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclh5u05ej20zk0m87df.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclgrvbd6j20zk0m8qv5.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1gicli9lfebj20zk0m84qp.jpg"></li><li class="item" data-background-image="https://tva1.sinaimg.cn/large/6833939bly1giclil3m4ej20zk0m8tn8.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="index wrap"><div class="segments posts"><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/note/" itemprop="url" title="NLP和transformer大类概述"><img data-src="https://tva1.sinaimg.cn/mw690/6833939bly1giph4lm9i7j20zk0m84qp.jpg"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>192</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/note/" itemprop="url" title="NLP和transformer大类概述">NLP和transformer大类概述</a></h3><div class="excerpt">对于 head 的理解，预训练的模型可能自己本身是带有 head 的，如果使用 autoModel 的话，那么就会自动加上这个 Head。但如果想要利用这个预训练模型，然后调整下游任务，那么就要换上特定的 token，这是就得使用 AutoModelFor...，这样模型就会自动替换原先的 head，然后就可以从头开始训练了。 但是因为还是使用原来的模型，所以 AutoTokenizer 还是可以继续用的。</div><div class="meta footer"><span><a href="/categories/ai/huggingface/" itemprop="url" title="huggingface"><i class="ic i-flag"></i>huggingface</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/note/" itemprop="url" title="NLP和transformer大类概述" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" itemprop="url" title="问答 question answer"><img data-src="https://tva1.sinaimg.cn/mw690/6833939bly1giciuv0socj20zk0m8qes.jpg"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>18k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>16 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" itemprop="url" title="问答 question answer">问答 question answer</a></h3><div class="excerpt"># 问答 Question answering 是时候看问答了！这项任务有多种形式，但我们将在本节中关注的一项称为提取的问答 extractive question answering。问题的答案就在 给定的文档 之中。 我们将使用 SQuAD 数据集 微调一个 BERT 模型，其中包括群众工作者对一组维基百科文章提出的问题。 像 BERT 这样的纯编码器模型往往很擅长提取诸如 “谁发明了 Transformer 架构？” 之类的事实性问题的答案。但在给出诸如 “为什么天空是蓝色的？” 之类的开放式问题时表现不佳。在这些更具挑战性的情况下，T5 和 BART 等编码器 -...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81nlp%E4%BB%BB%E5%8A%A1/" itemprop="url" title="主要nlp任务"><i class="ic i-flag"></i>主要nlp任务</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E9%97%AE%E7%AD%94/" itemprop="url" title="问答 question answer" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" itemprop="url" title="微调一个预训练模型-处理数据"><img data-src="https://tva1.sinaimg.cn/mw690/6833939bly1gipesrnqv3j20zk0m8ava.jpg"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>11k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>10 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" itemprop="url" title="微调一个预训练模型-处理数据">微调一个预训练模型-处理数据</a></h3><div class="excerpt"># 处理数据 下面是我们用模型中心的数据在 PyTorch 上训练句子分类器的一个例子： import torchfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification# Same as beforecheckpoint = &quot;bert-base-uncased&quot;tokenizer = AutoTokenizer.from_pretrained(checkpoint)model =...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" itemprop="url" title="微调一个预训练模型"><i class="ic i-flag"></i>微调一个预训练模型</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E5%BE%AE%E8%B0%83%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/" itemprop="url" title="微调一个预训练模型-处理数据" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" itemprop="url" title="Byte-Pair Encoding tokenization"><img data-src="https://tva1.sinaimg.cn/mw690/6833939bly1giclwrdwyaj20zk0m8are.jpg"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>7.3k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>7 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" itemprop="url" title="Byte-Pair Encoding tokenization">Byte-Pair Encoding tokenization</a></h3><div class="excerpt"># Byte-Pair Encoding tokenization 字节对编码 (BPE) 最初被开发为一种压缩文本的算法，然后在预训练 GPT 模型时被 OpenAI 用于标记化。许多 Transformer 模型都使用它，包括 GPT、GPT-2、RoBERTa、BART 和 DeBERTa。 # 训练算法 BPE 训练首先计算语料库中使用的唯一单词集 (在完成标准化和预标记化步骤之后), 然后通过获取用于编写这些单词的所有符号来构建词汇表。一个非常简单的例子，假设我们的语料库使用了这五个词: &quot;hug&quot;, &quot;pug&quot;,...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/BPE/" itemprop="url" title="Byte-Pair Encoding tokenization" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/NormalizationAndPre-tokenization/" itemprop="url" title="Normalization and pre-tokenization"><img data-src="https://tva1.sinaimg.cn/mw690/6833939bly1gipexj2jgzj20zk0m8b09.jpg"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>371</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/NormalizationAndPre-tokenization/" itemprop="url" title="Normalization and pre-tokenization">Normalization and pre-tokenization</a></h3><div class="excerpt"># Normalization and pre-tokenization 标准化和预标记化 在我们更深入地研究与 Transformer 模型（字节对编码 Byte-Pair Encoding [BPE]、WordPiece 和 Unigram）一起使用的三种最常见的子词标记化算法之前，我们将首先看一下每个标记器 tokenizer 应用于文本的预处理。以下是 tokenization pipeline 标记化管道 中步骤的高级概述： 在将文本拆分为子标记之前（根据其模型），分词器执行两个步骤： normalization 和 pre-tokenization. #...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/NormalizationAndPre-tokenization/" itemprop="url" title="Normalization and pre-tokenization" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81Summarization/" itemprop="url" title="文本摘要 summarize"><img data-src="https://tva1.sinaimg.cn/mw690/6833939bly1gipexbei4hj20zk0m8npd.jpg"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>6.1k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>6 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81Summarization/" itemprop="url" title="文本摘要 summarize">文本摘要 summarize</a></h3><div class="excerpt"># 文本摘要 在本节中，我们将看看如何使用 Transformer 模型将长文档压缩为摘要，这项任务称为文本摘要。这是最具挑战性的 NLP 任务之一，因为它需要一系列能力，例如理解长篇文章和生成能够捕捉文档中主要主题的连贯文本。但是，如果做得好，文本摘要是一种强大的工具，可以减轻领域专家详细阅读长文档的负担，从而加快各种业务流程。 尽管在 Hugging Face Hub 上已经存在各种微调模型用于文本摘要，几乎所有这些都只适用于英文文档。因此，为了在本节中添加一些变化，我们将为英语和西班牙语训练一个双语模型。在本节结束时，您将有一个可以总结客户评论的模型。 #...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81nlp%E4%BB%BB%E5%8A%A1/" itemprop="url" title="主要nlp任务"><i class="ic i-flag"></i>主要nlp任务</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/%E4%B8%BB%E8%A6%81%E7%9A%84%20NLP%20%E4%BB%BB%E5%8A%A1/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81Summarization/" itemprop="url" title="文本摘要 summarize" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/WordPiece/" itemprop="url" title="WordPiece"><img data-src="https://tva1.sinaimg.cn/mw690/6833939bly1gipex2cdtbj20zk0m8x6p.jpg"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>848</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/WordPiece/" itemprop="url" title="WordPiece">WordPiece</a></h3><div class="excerpt"># WordPiece 标记化 WordPiece 是 Google 为预训练 BERT 而开发的标记化算法。此后，它在不少基于 BERT 的 Transformer 模型中得到重用，例如 DistilBERT、MobileBERT、Funnel Transformers 和 MPNET。它在训练方面与 BPE 非常相似，但实际标记化的方式不同。 # 训练算法 Google 从未开源 WordPiece 训练算法的实现，因此以下是我们基于已发表文献的最佳猜测。它可能不是 100% 准确的。 与 BPE 一样，WordPiece...</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/WordPiece/" itemprop="url" title="WordPiece" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/Unigram%E6%A0%87%E8%AE%B0%E5%8C%96/" itemprop="url" title="Unigram"><img data-src="https://tva1.sinaimg.cn/mw690/6833939bly1gicitht3xtj20zk0m8k5v.jpg"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-12 12:04:17"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-12T12:04:17+08:00">2022-11-12</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>104</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/Unigram%E6%A0%87%E8%AE%B0%E5%8C%96/" itemprop="url" title="Unigram">Unigram</a></h3><div class="excerpt"># Unigram 标记化 Unigram tokenization 在 SentencePiece 中经常使用 Unigram 算法，该算法是 AlBERT、T5、mBART、Big Bird 和 XLNet 等模型使用的标记化算法。</div><div class="meta footer"><span><a href="/categories/ai/nlp/huggingface/Tokenizer%E5%BA%93/" itemprop="url" title="Tokenizer库"><i class="ic i-flag"></i>Tokenizer库</a></span></div><a href="/2022/11/12/ai/nlp/huggingface/Tokenizers%E5%BA%93/Unigram%E6%A0%87%E8%AE%B0%E5%8C%96/" itemprop="url" title="Unigram" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/11/computer-science/%E6%95%B0%E7%AB%9E/%E6%95%B0%E7%AB%9E/" itemprop="url" title="未命名"><img data-src="https://tva1.sinaimg.cn/mw690/6833939bly1gicitcxhpij20zk0m8hdt.jpg"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-11 00:15:22"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-11T00:15:22+08:00">2022-11-11</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>0</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/11/computer-science/%E6%95%B0%E7%AB%9E/%E6%95%B0%E7%AB%9E/" itemprop="url" title="未命名">未命名</a></h3><div class="excerpt"></div><a href="/2022/11/11/computer-science/%E6%95%B0%E7%AB%9E/%E6%95%B0%E7%AB%9E/" itemprop="url" title="未命名" class="btn">more...</a></div></article><article class="item"><div class="cover"><a href="/2022/11/09/ai/pytorch/MAML/" itemprop="url" title="未命名"><img data-src="https://tva1.sinaimg.cn/mw690/6833939bly1gipeu7txpzj20zk0m81kx.jpg"></a></div><div class="info"><div class="meta"><span class="item" title="创建时间：2022-11-09 21:28:37"><span class="icon"><i class="ic i-calendar"></i> </span><time itemprop="dateCreated datePublished" datetime="2022-11-09T21:28:37+08:00">2022-11-09</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span>43</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span>1 分钟</span></span></div><h3><a href="/2022/11/09/ai/pytorch/MAML/" itemprop="url" title="未命名">未命名</a></h3><div class="excerpt"># 参考资料 https://zhuanlan.zhihu.com/p/448715415</div><a href="/2022/11/09/ai/pytorch/MAML/" itemprop="url" title="未命名" class="btn">more...</a></div></article></div></div><nav class="pagination"><div class="inner"><a class="extend prev" rel="prev" href="/"><i class="ic i-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/20/">20</a><a class="extend next" rel="next" href="/page/3/"><i class="ic i-angle-right" aria-label="下一页"></i></a></div></nav></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"></div><div class="related panel pjax" data-title="系列文章"></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="yuan" data-src="/images/avatar.jpg"><p class="name" itemprop="name">yuan</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">193</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">45</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">39</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item email" data-url="bWFpbHRvOjIwODM2MzU1MjVAcXEuY29t" title="mailto:2083635525@qq.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/page/3/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch/" title="分类于 pytorch">pytorch</a></div><span><a href="/2022/08/24/ai/pytorch/torchvision-datasets-ImageFolder/" title="torchvision.datasets.ImageFolder">torchvision.datasets.ImageFolder</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/cv/" title="分类于 cv">cv</a></div><span><a href="/2022/08/25/ai/cv/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/" title="语义分割">语义分割</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2022/11/01/computer-science/%E9%AB%98%E7%BA%A7%E8%BD%AF%E8%80%83/%E9%80%89%E6%8B%A9%E9%A2%98/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2022/08/25/language/vbs/vbs/" title="vbs">vbs</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch/" title="分类于 pytorch">pytorch</a></div><span><a href="/2022/08/24/ai/pytorch/PyTorch%E5%85%B3%E4%BA%8E%E4%BB%A5%E4%B8%8B%E6%96%B9%E6%B3%95%E4%BD%BF%E7%94%A8%EF%BC%9Adetach-cpu-numpy-%E4%BB%A5%E5%8F%8Aitem/" title="PyTorch关于以下方法使用：detach()_cpu()_numpy()_以及item()">PyTorch关于以下方法使用：detach()_cpu()_numpy()_以及item()</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2022/10/17/computer-science/%E9%AB%98%E7%BA%A7%E8%BD%AF%E8%80%83/%E8%AE%BA%E6%96%87/%E5%87%86%E5%A4%87/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ai/" title="分类于 ai">ai</a> <i class="ic i-angle-right"></i> <a href="/categories/ai/pytorch/" title="分类于 pytorch">pytorch</a></div><span><a href="/2022/08/24/ai/pytorch/Pytorch%E4%B8%ADtransforms-RandomResizedCrop-%E7%AD%89%E5%9B%BE%E5%83%8F%E6%93%8D%E4%BD%9C/" title="Pytorch中transforms.RandomResizedCrop()等图像操作">Pytorch中transforms.RandomResizedCrop()等图像操作</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/backend/" title="分类于 后端">后端</a> <i class="ic i-angle-right"></i> <a href="/categories/backend/MyBatisPlus/" title="分类于 MyBatisPlus">MyBatisPlus</a></div><span><a href="/2022/11/12/backend/Mybatisplus/MyBatisPlus/" title="MyBatisPlus">MyBatisPlus</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/tools/" title="分类于 tools">tools</a></div><span><a href="/2022/07/26/tools/vscode%E6%8A%80%E5%B7%A7/" title="vscode技巧">vscode技巧</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/tools/" title="分类于 tools">tools</a> <i class="ic i-angle-right"></i> <a href="/categories/tools/%E7%88%AC%E8%99%AB/" title="分类于 爬虫">爬虫</a></div><span><a href="/2022/08/26/tools/%E7%88%AC%E8%99%AB/scrapy/" title="scrapy">scrapy</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">yuan @ Mi Manchi</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">1.4m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">21:33</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"page/2/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>